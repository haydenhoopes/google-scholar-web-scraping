{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "params = {           # These should be modified by the user\n",
    "    'year': '2020',\n",
    "    'filepath': '../Downloads/20190607_OCIS member list - Copy.xlsx',\n",
    "    'column_name': 'LastName, Name',\n",
    "    'output_name': 'publications.xlsx'\n",
    "}\n",
    "\n",
    "BASE_URI = 'https://scholar.google.com'\n",
    "\n",
    "list_of_authors = pd.DataFrame(pd.read_excel(params['filepath'], header=3))[params['column_name']].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Pair all the authors with their user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait():\n",
    "    time.sleep((8-5)*np.random.random()+5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user A Rebecca Reuber did not have a profile\n",
      "1 authors do not have profiles\n"
     ]
    }
   ],
   "source": [
    "# There is no way to know programmatically what an author's userId will be. However, to reduce the chance of including\n",
    "# articles written by other authors of the same name, the program will select the most popular user with the given name.\n",
    "# The userId can also be added manually.\n",
    "\n",
    "def getUserIds(list_of_authors):\n",
    "    auth_list_with_ids = []\n",
    "    authors_without_profile = []\n",
    "    count_without = 0 # get count of users with user profile\n",
    "    for author in list_of_authors:\n",
    "        ### Do stuff to get the id\n",
    "        # Format the author's name\n",
    "        name = author.replace(\", \", \"+\").replace(\" \", \"+\")\n",
    "        \n",
    "        # Google the author's name\n",
    "        URI = BASE_URI + \"/citations?view_op=search_authors&mauthors={}&hl=en&oi=ao\"\n",
    "        URL = URI.format(name)\n",
    "        pattern = '(?<=user=)(.*)'\n",
    "        wait()\n",
    "        \n",
    "        # Get the href of the a tag that goes to the profile page\n",
    "        r = requests.get(URL).text\n",
    "        soup = BeautifulSoup(r, 'html.parser')\n",
    "        user = soup.find(\"h3\")\n",
    "        if user is None:\n",
    "            count_without += 1\n",
    "            authors_without_profile.append(author)\n",
    "            print(f\"user {author} did not have a profile\")\n",
    "#             raise Exception(f'No user was found. You may need to locate the User Id Manually for \\'{author}\\'')\n",
    "        else:    \n",
    "            user_url= user.a['href']\n",
    "\n",
    "            # Extract the id from the href url\n",
    "            scholarId = re.search(pattern, user_url).group(0)\n",
    "\n",
    "            auth_list_with_ids.append({'name': author, 'scholarId': scholarId})\n",
    "    print(f\"{count_without} authors do not have profiles\")    \n",
    "    return auth_list_with_ids, authors_without_profile\n",
    "\n",
    "author_list, authors_without_profile = getUserIds(list_of_authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Go through each authors' individual page, scraping the required data into a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request sent\t1: Request successful\n"
     ]
    }
   ],
   "source": [
    "def getAuthorData(author_list):\n",
    "    data = [] # While looping through authors and their publications, add data to list\n",
    "    count = 0\n",
    "    for author in author_list:\n",
    "        # Make the URL, using the author's scholarId\n",
    "        URI = BASE_URI + '/citations?hl=en&user={}&view_op=list_works&sortby=pubdate'\n",
    "        URL = URI.format(author['scholarId'])\n",
    "        \n",
    "        \n",
    "        # Get the page in HTML        \n",
    "        # Google blocking: error handling\n",
    "        req_finished = False\n",
    "        while req_finished is False:\n",
    "            wait()\n",
    "            print(\"Request sent\", end=\"\\t\")\n",
    "            r = requests.get(URL).text\n",
    "            soup = BeautifulSoup(r, 'html.parser')\n",
    "            \n",
    "            if soup.find(\"tbody\", id='gsc_a_b') is None: # if the request is still blocked for some reason\n",
    "                print(\"The request was blocked. Will try request again in two hours\")\n",
    "                time.sleep(7210) # wait for about two hours for Google to stop blocking and try again\n",
    "            else:\n",
    "                count += 1\n",
    "                req_finished = True\n",
    "                print(f\"{count}: Request successful\")\n",
    "        \n",
    "        # Get each table row of publication and filter by the given year\n",
    "        for publication in soup.find(\"tbody\", id='gsc_a_b').find_all(\"tr\"):\n",
    "            year = publication.find(\"td\", class_='gsc_a_y').span.text\n",
    "\n",
    "\n",
    "        # Get HTML of publications by year\n",
    "            if year == params['year']:\n",
    "                r = requests.get(BASE_URI + publication.find('a')['data-href']).text\n",
    "                psoup = BeautifulSoup(r, 'html.parser')\n",
    "\n",
    "        # If the publication was not a conference, keep going\n",
    "                fields_keep = ['authors', 'publication_date', 'title', 'journal', 'volume', 'issue', 'pages', 'publisher']\n",
    "                publication_entry = {}\n",
    "                for k in psoup.find_all(\"div\", class_=\"gs_scl\"):\n",
    "                    field = k.find('div', class_='gsc_vcd_field').text.lower().replace(\" \", \"_\")\n",
    "                    value = k.find('div', class_='gsc_vcd_value').text\n",
    "\n",
    "                    if field in fields_keep:\n",
    "                        publication_entry[field] = value\n",
    "                        \n",
    "                title = psoup.find('a', class_='gsc_vcd_title_link')\n",
    "                if title is None:\n",
    "                    title = psoup.find('div', id='gsc_vcd_title')\n",
    "                publication_entry['title'] = title.text\n",
    "\n",
    "        # Append the data entry to the data\n",
    "                p_keys = publication_entry.keys()\n",
    "                if \"conference\" not in p_keys and 'journal' in p_keys: # and 'volume' in p_keys and 'issue' in p_keys:\n",
    "                    data.append(publication_entry)\n",
    "    return data\n",
    "\n",
    "data = getAuthorData(author_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Get data for authors without a Google Scholar profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1: Author A Rebecca Reuber was processed.\n"
     ]
    }
   ],
   "source": [
    "# This step is to get publications data for members who do not have a Google Scholar profile. These results may\n",
    "# be completely erroneous since they are matching members to articles based on name only and may contain data that\n",
    "# is not relevant. Hopefully filtering the results by the same keywords as used above will allow some correct\n",
    "# articles to be included which weren't included before.\n",
    "\n",
    "# We already got the list of authors without a profile before --> authors_without_profile\n",
    "pattern = '(?<=user=).+?(?=&)'\n",
    "num_authors = len(authors_without_profile)\n",
    "count = 0\n",
    "\n",
    "for author in authors_without_profile:\n",
    "    # Google the author's name\n",
    "    URL = f\"{BASE_URI}/scholar?q={author.replace(' ', '+')}&hl=en&as_sdt=0%2C45&as_ylo={params['year']}&as_yhi={params['year']}\"\n",
    "    \n",
    "    # Get the href of the a tag that goes to the profile page\n",
    "    r = requests.get(URL).text\n",
    "    wait()\n",
    "    soup = BeautifulSoup(r, 'html.parser')\n",
    "    for publication in soup.find_all(class_=\"gs_r gs_or gs_scl\"): # Get all of the articles that appear when you search the author's name\n",
    "        \n",
    "        article_title = publication.find(\"h3\").find(\"a\").text # This will be used later to get the article from a profile\n",
    "        \n",
    "        if publication.find(class_=\"gs_a\").find('b') is not None:\n",
    "            linked_authors_with_a_profile = [re.search(pattern, la['href']).group(0) for la in publication.find(class_=\"gs_a\").find_all('a')]\n",
    "            \n",
    "            # We just got all of the authors with a scholar profile who were coauthors with the current author.\n",
    "            # The next step is to go through the list of those profiled authors and see if they have information \n",
    "            # about the publication on their profile page. Sometimes this doesn't work because this program can't\n",
    "            # load more items to a page so if the article doesn't show up on the first page, we'll try the next\n",
    "            # profiled author.\n",
    "            \n",
    "            for a in linked_authors_with_a_profile:\n",
    "                URL = f'{BASE_URI}/citations?hl=en&user={a}&view_op=list_works&sortby=pubdate'\n",
    "                rr = requests.get(URL).text\n",
    "                wait()\n",
    "                asoup = BeautifulSoup(rr, 'html.parser')\n",
    "                \n",
    "                # Get each table row of publication and filter by the given year\n",
    "                link_child = asoup.find(text=article_title)\n",
    "                if link_child is not None:\n",
    "                    link = link_child.parent['data-href']\n",
    "\n",
    "                    # Get HTML of publications by year\n",
    "                    rrr = requests.get(BASE_URI + link).text\n",
    "                    wait()\n",
    "                    psoup = BeautifulSoup(rrr, 'html.parser')\n",
    "\n",
    "                # If the publication was not a conference, keep going\n",
    "                    fields_keep = ['authors', 'publication_date', 'title', 'journal', 'volume', 'issue', 'pages', 'publisher']\n",
    "                    publication_entry = {}\n",
    "                    for k in psoup.find_all(\"div\", class_=\"gs_scl\"):\n",
    "                        field = k.find('div', class_='gsc_vcd_field').text.lower().replace(\" \", \"_\")\n",
    "                        value = k.find('div', class_='gsc_vcd_value').text\n",
    "\n",
    "                        if field in fields_keep:\n",
    "                            publication_entry[field] = value\n",
    "\n",
    "                    title = psoup.find('a', class_='gsc_vcd_title_link')\n",
    "                    if title is None:\n",
    "                        title = psoup.find('div', id='gsc_vcd_title')\n",
    "                    publication_entry['title'] = title.text\n",
    "\n",
    "            # Append the data entry to the data\n",
    "                    p_keys = publication_entry.keys()\n",
    "                    if \"conference\" not in p_keys: # and 'volume' in p_keys and 'issue' in p_keys:\n",
    "                        data.append(publication_entry)\n",
    "                        break\n",
    "    count += 1                    \n",
    "    print(f\"{count}/{num_authors}: Author {author} was processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Validate and clean the data by keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data).drop_duplicates(subset=['title', 'pages'])\n",
    "\n",
    "\n",
    "keywords = ['data', 'information', 'organization', 'system', 'technology', 'software', \n",
    "            'digital', 'business', ' mis ', 'decision', 'code'] #This list is very important\n",
    "\n",
    "df.dropna(inplace=True, subset=['journal'])\n",
    "df['new_title'] = df['title'].apply(lambda x: None if not any(kw in x.lower() for kw in keywords) else x)\n",
    "df['new_journal'] = df['journal'].apply(lambda x: None if not any(kw in x.lower() for kw in keywords) else x)\n",
    "\n",
    "notkeywords = ['quarterly', 'conference', 'proceedings', 'cardiology', 'health', 'heart', 'medical', 'mammal', 'psychology', 'dermatology', 'disease', 'cell', 'tissue', 'medicine', 'oncology', 'astronomy', 'cystic', 'fibrosis']\n",
    "df.dropna(inplace=True, subset=['new_journal', 'new_title'], how='all')\n",
    "df['title'] = df['title'].apply(lambda x: None if any(nkw in x.lower() for nkw in notkeywords) else x)\n",
    "df['journal'] = df['journal'].apply(lambda x: None if any(nkw in x.lower() for nkw in notkeywords) else x)\n",
    "\n",
    "df.dropna(inplace=True, subset=['journal', 'title'])\n",
    "\n",
    "df.drop(['new_title', 'new_journal'], inplace=True, axis=1)\n",
    "print(\"Data successfully validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export to Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-52a1abaf2f2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-16\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabsolute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Data exported successfully to {pth}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_excel(params['output_name'], encoding=\"utf-16\", index=False)\n",
    "pth = f\"{str(pathlib.Path(params['output_name']).parent.absolute())}/{params['output_name']}\".replace(\"\\\\\", '/')\n",
    "print(f\"Data exported successfully to {pth}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
